{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torchvision \n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from modules_th.video_block import *\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.43216, 0.394666, 0.37645]\n",
    "std = [0.22803, 0.22145, 0.216989]\n",
    "\n",
    "model = create_body(torchvision.models.video.r2plus1d_18, cut=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_files</th>\n",
       "      <th>lbls</th>\n",
       "      <th>val</th>\n",
       "      <th>embs_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264.mp4</td>\n",
       "      <td>Testing_Normal_Videos_Anomaly</td>\n",
       "      <td>False</td>\n",
       "      <td>/mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               vid_files  \\\n",
       "0  /mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264.mp4   \n",
       "\n",
       "                            lbls    val  \\\n",
       "0  Testing_Normal_Videos_Anomaly  False   \n",
       "\n",
       "                                                                                           embs_path  \n",
       "0  /mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_df = '/mnt/data/eugeniomarinelli/UCF_experiments/train_UCF.csv'\n",
    "df = pd.read_csv('/mnt/data/eugeniomarinelli/UCF_experiments/train_UCF.csv', index_col=0)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_paths = df.vid_files.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2884.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frame_to_snip(res, frame, snip, j):\n",
    "    if res: \n",
    "        snip[j] = tensor(frame)\n",
    "        j += 1\n",
    "    return snip, j\n",
    "\n",
    "def snip_is_ready(j, sl):\n",
    "    return j >= sl\n",
    "\n",
    "def create_and_save_embs(snip, pl, embs_path, k):\n",
    "    emb = model(pl(snip)[None])\n",
    "#     torch.save(emb, embs_path/f'{embs_path.name}_{k}.pt') # save embeddings  \n",
    "    return k + 1\n",
    "\n",
    "def get_embs_of_video(vid_path, sl, size):    \n",
    "    vidcap = cv2.VideoCapture(str(random.choice(vid_paths)))\n",
    "    duration = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        n_loaded_frames, n_created_embs = 0, 0  # j counts # of frames loaded mod sl, k counts # of snippets\n",
    "        snip = torch.zeros((sl,240,320,3), dtype=torch.uint8)\n",
    "        for i in range(duration):\n",
    "            if duration - i < sl: break\n",
    "            res, frame = vidcap.read()\n",
    "            snip, j = add_frame_to_snip(res, frame, snip, j)\n",
    "            if snip_is_ready(j, sl): \n",
    "                k = create_and_save_embs(snip, pl, '', k)\n",
    "                j = 0\n",
    "\n",
    "    vidcap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.89058896782808"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl, size = 16, 60, 224\n",
    "pl = Pipeline([Video.create, \n",
    "               Resize(size,method=ResizeMethod.Pad), \n",
    "               ToTensor(), \n",
    "               IntToFloatTensor(), \n",
    "               Normalize.from_stats(mean,std)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.333333333333333"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "440/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8697916666666667"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration/sl/bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-ea47d2630e30>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.embs_path[j] = str(embs_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23nt/data/eugeniomarinelli/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos384_x264\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98nt/data/eugeniomarinelli/UCF_Crimes/Videos/Robbery/Robbery086_x264Anomaly/Normal_Videos501_x264\r"
     ]
    }
   ],
   "source": [
    "bs, sl, size = 16, 60, 224\n",
    "pl = Pipeline([Video.create, \n",
    "               Resize(size,method=ResizeMethod.Pad), \n",
    "               ToTensor(), \n",
    "               IntToFloatTensor(), \n",
    "               Normalize.from_stats(mean,std)])\n",
    "\n",
    "for j, path in enumerate(vid_paths):\n",
    "#        if isinstance(df.embs_path.values[j],str) : continue; # start where we left\n",
    "    print(j, end='\\r')\n",
    "    embs_path = Path(path[:-4]) # path to save emb video snippets \n",
    "    if not os.path.isdir(embs_path): os.mkdir(embs_path) # make folder\n",
    "\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    duration = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        j, k = 0, 0  # j counts # of frames loaded mod sl, k counts # of snippets\n",
    "        snip = torch.zeros((sl,240,320,3), dtype=torch.uint8)\n",
    "        for i in range(duration):\n",
    "            if duration - i < sl: break\n",
    "            res, frame = vidcap.read()\n",
    "            snip, j = add_frame_to_snip(res, frame, snip, j)\n",
    "            if snip_is_ready(j, sl): \n",
    "                k = create_and_save_embs(snip, pl, '', k)\n",
    "                j = 0\n",
    "\n",
    "    vidcap.release()\n",
    "            df.embs_path[j] = str(embs_path)\n",
    "            df.to_csv(path_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1mnt/data/eugeniomarinelli/UCF_Crimes/Videos/Testing_Normal_Videos_Anomaly/Normal_Videos_903_x264\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-ea47d2630e30>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.embs_path[j] = str(embs_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23nt/data/eugeniomarinelli/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos384_x264\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n",
      "mb_type 61 in P slice too large at 13 9\n",
      "error while decoding MB 13 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98nt/data/eugeniomarinelli/UCF_Crimes/Videos/Robbery/Robbery086_x264Anomaly/Normal_Videos501_x264\r"
     ]
    }
   ],
   "source": [
    "sl = 32\n",
    "bs = 8\n",
    "int2f = IntToFloatTensor()\n",
    "pl = Pipeline([Resize(224, method=ResizeMethod.Pad),ToTensor(),IntToFloatTensor(), Normalize.from_stats(mean,std)])\n",
    "with torch.no_grad(): \n",
    "    for j, path in enumerate(vid_paths):\n",
    "#        if isinstance(df.embs_path.values[j],str) : continue; # start where we left\n",
    "        print(j, end='\\r')\n",
    "        embs_path = Path(path[:-4]) # path to save emb video snippets \n",
    "        if not os.path.isdir(embs_path): os.mkdir(embs_path) # make folder\n",
    "        times = L(L(), L(), L())\n",
    "        i = 0\n",
    "        for start in range(0, int(duration)-sl*bs, sl*bs):\n",
    "            l = duration - duration%(sl*bs) if duration < sl*bs else sl*bs\n",
    "            \n",
    "            t0 = time.perf_counter()\n",
    "            snips = create_video(path, start = start, l=l, skip=1, form='img')\n",
    "            times[0].append(time.perf_counter()-t0)\n",
    "            \n",
    "            snips = L(chunked(snips, sl)).map(Video).map(pl).stack()       \n",
    "            \n",
    "            t1 = time.perf_counter()\n",
    "            embs = model(snips.cuda())\n",
    "            times[1].append(time.perf_counter()-t1)\n",
    "            \n",
    "            t2 = time.perf_counter()\n",
    "            for emb in embs:\n",
    "                torch.save(emb, embs_path/f'{embs_path.name}_{i}.pt') # save embeddings\n",
    "                i +=1\n",
    "            times[2].append(time.perf_counter()-t2)\n",
    "            \n",
    "        print(embs_path, end='\\r')\n",
    "        df.embs_path[j] = str(embs_path)\n",
    "        df.to_csv(path_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normal_Videos437_x264.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_files('/mnt/data/eugeniomarinelli/UCF_Crimes/Video', extensions='.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 32, 224, 224)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(chunked(snips, sl)).map(Video).map(pl).stack().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#76600) [None,None,None,None,None,None,None,None,None,None...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_files('/mnt/data/eugeniomarinelli/UCF_Crimes/Videos',extensions='.pt')\n",
    "files.map(os.remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
