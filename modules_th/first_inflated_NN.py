# AUTOGENERATED! DO NOT EDIT! File to edit: 06_distributed_inflated_NN.ipynb (unless otherwise specified).

__all__ = []

# Cell
import torch
import torch.nn as nn
from fastai.vision.all import *
from IPython.display import display, clear_output
from fastai.data.all import *
from fastai.distributed import *
from fastscript import *
import pandas as pd
from pathlib import Path
import time
from video_block import *
from inflator import *
from triplet_loss import *
import warnings

from charades import *

# Internal Cell
@call_parse
def main(gpu    :Param("GPU to run on", int)=None,
         eugenio:Param("Variable to control whether is Eugenio or me that's calling the training", bool)=False,
         n_lbl  :Param("# of different labels per batch", int)=8,
         n_el   :Param("# of elements per label", int)=4,
         l      :Param("Target number of frames of the ResizeTime transform", int)=60,
         sz     :Param("size for Resize", int)=224,
         n_epoch:Param("# of epochs to train", int)=8,
        ):

    if gpu is not None:
        gpu = setup_distrib(gpu)
        items = rank0_first(lambda: read_data(eugenio))
    else:
        items = read_data(eugenio)

    learn = get_learner(items, l, sz, n_lbl, n_el)


    torch.cuda.empty_cache()
    if gpu is not None:
        print("Distributed Data Parallel training started")
        with learn.distrib_ctx(gpu):
            learn.fine_tune(n_epoch)
    else:
        warnings.filterwarnings("ignore", message='.*nonzero.*', category=UserWarning)
        print("Data Parallel training started")
        with learn.parallel_ctx(device_ids=[0,1]):
            learn.fine_tune(n_epoch)

    learn.save('trained_models/Inflated1')