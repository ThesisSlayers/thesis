# AUTOGENERATED! DO NOT EDIT! File to edit: distributed_training.ipynb (unless otherwise specified).

__all__ = ['main']

# Cell
import torch
import torch.nn as nn
from fastai.vision.all import *
from IPython.display import display, clear_output
from fastai.data.all import *
from fastai.distributed import *
from fastscript import *
import pandas as pd
from pathlib import Path
import time
import warnings
from datetime import datetime

# Cell
from .video_block import *
from .inflator import *
from .triplet_loss import *
from .supcon_module import *
from .cus_cbs import *
from .ucf_cnn import *

# Cell
@call_parse
def main(gpu    :Param("GPU to run on", int)=None,
         file   :Param("csv path", bool)='UCF_experiments/train_UCF.csv',
         n_lbl  :Param("# of different labels per batch", int)=4,
         n_el   :Param("# of elements per label", int)=2,
         l      :Param("Target number of frames of the ResizeTime transform", int)=60,
         skip   :Param("skip frames",int)=20,
         size     :Param("size for Resize", int)=224,
         n_epoch:Param("# of epochs to train", int)=10,
         loss   :Param("Choose a loss between CEL-SCL and SCL ", str)='CEL-SCL',
         embs_size:Param("embeddings size", int)=128,
         n_views:Param("number of views", int)=2,
         log_nm :Param("logs file name. if 'date' the name will be composed by the date", str)='date',
         descr  :Param("description of the experiment", str)='First training with mixed SupConLoss to have a preformance baseline; first trial with a large skip'
        ):
    prefix = '/mnt/data/eugeniomarinelli/'

    if gpu is not None:
        gpu = setup_distrib(gpu)
        items = rank0_first(read_data)
    else:
        items = read_data()

    learn = get_learner(items, loss, l, size, n_lbl , n_el, skip, embs_size,n_views)
    print('Learner Loaded \n')


    # set up logs file
    now = datetime.now()
    time = now.strftime("%d_%m_h%H")
    logs_file = prefix+'logs_'+time+'.csv' if log_nm == 'date' else prefix+'logs_'+log_nm+'.csv'
    Logs_csv =   CSVLogger(fname= logs_file, append=False)

    torch.cuda.empty_cache()
    if gpu is not None:
        print("Distributed Data Parallel training started")
        with learn.distrib_ctx(gpu):
            learn.fine_tune(n_epoch,cbs=[Logs_csv])
    else:
        warnings.filterwarnings("ignore", message='.*nonzero.*', category=UserWarning)
        print("Data Parallel training started")
        with learn.parallel_ctx(device_ids=[0,1]):
            learn.fine_tune(n_epoch,cbs=[Logs_csv])

    learn.save(prefix+'UCF_experiments/trained_models_cnn/ucf_cnn_'+time)
    save_CEL_SCL_losses(learn, logs_file)
    experiment = pd.DataFrame({'date':time,
             'description':descr,
             'l':l,
             'skip':skip,
             'size':size,
             'loss':loss,
             'n_el':n_el,
             'n_lbl':n_lbl,
             'n_epochs':n_epoch}).to_csv(prefix+'UCF_experiments/experiments_logs.csv', mode='a')
